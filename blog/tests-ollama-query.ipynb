{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama native API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "options={\n",
    "    'num_ctx': 4096,\n",
    "    'temperature': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scattered sunlight by tiny molecules in Earth's atmosphere makes it blue.\n",
      "Scattered sunlight by tiny molecules in Earth's atmosphere makes it blue.\n"
     ]
    }
   ],
   "source": [
    "# Synchronous\n",
    "\n",
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "response: ChatResponse = chat(model='llama3.1', options=options, messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue? Answer with about 10 words.',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])\n",
    "# or access fields directly from the response object\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**The Color of the Sky**\n",
      "Scattering of sunlight by atmospheric particles makes it blue."
     ]
    }
   ],
   "source": [
    "# Synchronous with stream=True\n",
    "\n",
    "from ollama import chat\n",
    "\n",
    "stream = chat(\n",
    "    model='llama3.1',\n",
    "    messages=[{'role': 'user', 'content': 'Why is the sky blue? Answer with about 10 words in markdown with a title'}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatResponse(model='llama3.1', created_at='2025-04-11T12:42:16.632337956Z', done=True, done_reason='stop', total_duration=829041452, load_duration=18770417, prompt_eval_count=23, prompt_eval_duration=98000000, eval_count=14, eval_duration=710000000, message=Message(role='assistant', content='Scattered sunlight interacts with tiny molecules of atmospheric gases and particles.', images=None, tool_calls=None))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Synchronous client\n",
    "\n",
    "from ollama import Client\n",
    "client = Client(\n",
    "  host='http://localhost:11434',\n",
    "  headers={'x-some-header': 'some-value'}\n",
    ")\n",
    "response = client.chat(model='llama3.1', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue? Answer with about 10 words.',\n",
    "  },\n",
    "])\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='llama3.1' created_at='2025-04-11T12:42:17.422704645Z' done=True done_reason='stop' total_duration=681027206 load_duration=17601899 prompt_eval_count=23 prompt_eval_duration=69000000 eval_count=12 eval_duration=592000000 message=Message(role='assistant', content='The sky appears blue due to scattering of sunlight particles.', images=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# Asynchronous client\n",
    "\n",
    "import asyncio\n",
    "from ollama import AsyncClient\n",
    "\n",
    "async def chat():\n",
    "  message = {'role': 'user', 'content': 'Why is the sky blue? Answer with about 10 words.'}\n",
    "  return await AsyncClient().chat(model='llama3.1', messages=[message])\n",
    "\n",
    "#asyncio.run(chat())\n",
    "print(await chat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='llama3.1' created_at='2025-04-11T12:42:18.1623914Z' done=True done_reason='stop' total_duration=639514893 load_duration=17534501 prompt_eval_count=23 prompt_eval_duration=71000000 eval_count=11 eval_duration=549000000 message=Message(role='assistant', content='The sky appears blue due to scattering of sunlight.', images=None, tool_calls=None)\n",
      "model='llama3.1' created_at='2025-04-11T12:42:18.744962805Z' done=True done_reason='stop' total_duration=512966919 load_duration=18919492 prompt_eval_count=17 prompt_eval_duration=103000000 eval_count=8 eval_duration=389000000 message=Message(role='assistant', content='The capital of France is Paris.', images=None, tool_calls=None)\n",
      "model='llama3.1' created_at='2025-04-11T12:42:27.47907397Z' done=True done_reason='stop' total_duration=8659086357 load_duration=19973295 prompt_eval_count=17 prompt_eval_duration=108000000 eval_count=154 eval_duration=8529000000 message=Message(role='assistant', content=\"The speed of light in a vacuum is approximately 299,792,458 meters per second (m/s). This is a fundamental constant in physics and is denoted by the letter c. It's often expressed as:\\n\\nc = 299,792,458 m/s â‰ˆ 186,282 miles per second\\n\\nThis value has been extensively measured and verified to be incredibly precise. In fact, it's one of the most precisely known physical constants!\\n\\nInterestingly, the speed of light in a vacuum is always constant, regardless of the observer's frame of reference or their motion relative to the light source. This constancy is a fundamental aspect of Einstein's theory of special relativity.\\n\\nWould you like to know more about the speed of light or its implications in physics?\", images=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# Asynchronous with Semaphore limit to 1 and dynamic message addition with asyncio.Queue\n",
    "\n",
    "import asyncio\n",
    "from ollama import AsyncClient\n",
    "\n",
    "# Define a semaphore with a limit of 1\n",
    "semaphore = asyncio.Semaphore(1)\n",
    "\n",
    "async def chat(queue):\n",
    "    while True:\n",
    "        message = await queue.get()\n",
    "        if message is None:  # Sentinel value to stop the worker\n",
    "            break\n",
    "        async with semaphore:\n",
    "            response = await AsyncClient().chat(model='llama3.1', messages=[message])\n",
    "            print(response)\n",
    "        queue.task_done()\n",
    "\n",
    "async def main():\n",
    "    queue = asyncio.Queue()\n",
    "    \n",
    "    # Start the chat worker\n",
    "    worker = asyncio.create_task(chat(queue))\n",
    "    \n",
    "    # Add initial messages to the queue\n",
    "    await queue.put({'role': 'user', 'content': 'Why is the sky blue? Answer with about 10 words.'})\n",
    "    await queue.put({'role': 'user', 'content': 'What is the capital of France?'})\n",
    "    \n",
    "    # Dynamically add a third message after some delay\n",
    "    await asyncio.sleep(1)\n",
    "    await queue.put({'role': 'user', 'content': 'What is the speed of light?'})\n",
    "    \n",
    "    # Wait for all messages to be processed\n",
    "    await queue.join()\n",
    "    \n",
    "    # Stop the worker\n",
    "    await queue.put(None)\n",
    "    await worker\n",
    "\n",
    "# Run the main function\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky appears blue due to scattered sunlight particles."
     ]
    }
   ],
   "source": [
    "# Asynchronous client with stream=True\n",
    "\n",
    "from ollama import AsyncClient\n",
    "\n",
    "async def chat():\n",
    "  message = {'role': 'user', 'content': 'Why is the sky blue? Answer with about 10 words.'}\n",
    "  async for part in await AsyncClient().chat(model='llama3.1', messages=[message], stream=True):\n",
    "    print(part['message']['content'],end='', flush=True)\n",
    "\n",
    "#asyncio.run(chat())\n",
    "await chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scattering of shorter wavelengths by atmospheric particles makes it appear blue to human eyes."
     ]
    }
   ],
   "source": [
    "# Asynchronous client with stream=True and a \"yield\" instead of the \"print\"\n",
    "\n",
    "import asyncio\n",
    "from ollama import AsyncClient\n",
    "\n",
    "async def chat():\n",
    "  message = {'role': 'user', 'content': 'Why is the sky blue? Answer with about 10 words.'}\n",
    "  async for part in await AsyncClient().chat(model='phi4-mini', messages=[message], stream=True):\n",
    "    yield (part['message']['content'])\n",
    "\n",
    "async for result in chat():\n",
    "\tprint(result, end='',  flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to the COVID-19 pandemic, the 2020 World Series was played at Globe Life Field in Arlington, Texas, home of the Texas Rangers, and not at the Dodgers' typical home stadium, Dodger Stadium in Los Angeles. The games were held without a live audience due to health and safety protocols at that time.\n"
     ]
    }
   ],
   "source": [
    "# Synchronous\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='dummy_key', # required, but unused\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"llama3.1\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The LA Dodgers won in 2020.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-474', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Scattering of sunlight by tiny molecules makes the sky blue.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1744375376, model='llama3.1', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=CompletionUsage(completion_tokens=13, prompt_tokens=23, total_tokens=36, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "# Asynchronous client\n",
    "\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "client = AsyncOpenAI(\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='dummy_key', # required, but unused\n",
    ")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    chunks = await client.chat.completions.create(\n",
    "        model=\"llama3.1\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Why is the sky blue? Answer with about 10 words.\"}],\n",
    "    )\n",
    "    print(chunks)\n",
    "\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2"
     ]
    }
   ],
   "source": [
    "# Asynchronous client with stream=True\n",
    "\n",
    "import asyncio\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "client = AsyncOpenAI(\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='dummy_key', # required, but unused\n",
    ")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    stream = await client.chat.completions.create(\n",
    "        model=\"llama3.1\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"1+1=? Answer with just one number.\"}],\n",
    "        stream=True,\n",
    "    )\n",
    "    async for chunk in stream:\n",
    "        print(chunk.choices[0].delta.content or \"\", end=\"\", flush=True)\n",
    "\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scattered sunlight by tiny molecules in the atmosphere creates blue color."
     ]
    }
   ],
   "source": [
    "# Asynchronous client with stream=True and a \"yield\" instead of the \"print\"\n",
    "\n",
    "import asyncio\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "client = AsyncOpenAI(\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='dummy_key', # required, but unused\n",
    ")\n",
    "\n",
    "\n",
    "async def chat():\n",
    "    stream = await client.chat.completions.create(\n",
    "        model=\"llama3.1\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Why is the sky blue? Answer with about 10 words.\"}],\n",
    "        stream=True,\n",
    "    )\n",
    "    async for chunk in stream:\n",
    "        yield chunk.choices[0].delta.content\n",
    "\n",
    "\n",
    "async for result in chat():\n",
    "\tprint(result or \"\", end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphrag-GwOyU4gH-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
